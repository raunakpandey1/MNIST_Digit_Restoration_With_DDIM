# DDIM Inpainting on MNIST (Center-Hole)
Train a **timestep-conditioned U-Net** diffusion model on **masked MNIST** and reconstruct missing pixels using **DDIM sampling**. Evaluation is computed **only inside the masked hole** using **hole-only PSNR** and **hole-only L1 (MAE)**.

> Recommended: run on **Google Colab (GPU)** with a modest batch size to avoid OOM.

---

## Project Overview
This project implements a complete diffusion inpainting pipeline for **28×28 MNIST**:
1. Create a **fixed center-hole mask** (e.g., 14×14) and corrupt images by removing pixels inside the hole.
2. Train a diffusion model to predict noise (or x0 depending on your setup) with a **U-Net conditioned on timestep `t`**.
3. Perform **DDIM sampling** to reconstruct the missing region while keeping known pixels consistent.
4. Report **PSNR and L1 only inside the hole** (the only region that matters for inpainting quality).

---

## Key Features
- **Center-hole inpainting** on MNIST (fixed square mask).
- **Timestep-conditioned U-Net** (time embeddings + encoder/decoder blocks).
- **Forward diffusion** with configurable noise schedule (linear/cosine).
- **DDIM sampling** (fast, optionally deterministic with `eta=0`).
- **Mask-consistency step** during sampling: known pixels are preserved each step.
- **Hole-only metrics**:
  - **PSNR on masked region**
  - **L1 / MAE on masked region**
- Clean, modular structure: dataset, model, scheduler, sampling, evaluation.

---

## Methodology

### 1) Data + Masking
- Dataset: **MNIST**, normalized to **[0, 1]** (or `[-1, 1]` if you prefer, but keep evaluation consistent).
- Mask: a binary mask `M` where:
  - `M = 1` inside the hole (pixels to inpaint)
  - `M = 0` outside the hole (known pixels)
- Input to the model is typically the **noisy image** at timestep `t`.  
- During sampling, we enforce:
  - **known region** stays equal to the original observed pixels
  - **hole region** is generated by the diffusion model

### 2) Diffusion Process
We define a noise schedule `{β_t}`:
- `α_t = 1 - β_t`
- `\bar{α}_t = ∏_{s=1..t} α_s`

Forward noising:
- `x_t = sqrt(\bar{α}_t) * x_0 + sqrt(1 - \bar{α}_t) * ε`, with `ε ~ N(0, I)`

### 3) Model (Timestep-Conditioned U-Net)
- A lightweight U-Net suitable for 28×28 images.
- Inject timestep conditioning using a **sinusoidal embedding** + MLP.
- The model predicts either:
  - **noise** `ε_θ(x_t, t)` (most common), or
  - **x0** `x̂_0` (alternative parameterization)

Training loss (noise-prediction example):
- `L = || ε - ε_θ(x_t, t) ||_2^2`

### 4) DDIM Sampling for Inpainting
DDIM updates skip the stochasticity of DDPM (fewer steps, faster):
- Deterministic when `η = 0`
- Faster reconstructions while preserving quality for simple datasets like MNIST

**Mask-consistency step (core of inpainting):**  
At every reverse step, after proposing `x_{t-1}`, overwrite known pixels:
- `x_{t-1} = (1 - M) ⊙ x_obs_noisy(t-1) + M ⊙ x_{t-1}`
Where `x_obs_noisy(t-1)` is the **forward-noised** version of the observed image at that timestep (so noise level matches the current reverse step).

---

## Evaluation (Hole-Only Metrics)

### Hole-only L1 (MAE)
Compute error only where `M=1`:
- `L1_hole = mean( |x_pred - x_gt| * M ) / mean(M)`

### Hole-only PSNR
Using MSE over the hole region:
- `MSE_hole = mean( (x_pred - x_gt)^2 * M ) / mean(M)`
- `PSNR_hole = 20 * log10(MAX_I) - 10 * log10(MSE_hole)`
Where:
- `MAX_I = 1.0` if images are in `[0, 1]`


1. Machine & Environment Information
    Hardware
        Device : Macbook Air M1
        GPU: NVIDIA A100-SXM4-40GB
        GPU Memory: 40 GB HBM2e
        CUDA Availability: Yes
        CPU: Apple M1 8-core CPU
        RAM : RAM: 8GB

    Software
        OS: macOS
        Python: 3.12.12
        PyTorch: 2.9.0+cu126
        CUDA Toolkit: 12.6
        Torch CUDA: Available 

    Python packages used
        torch, torchvision
        numpy
        matplotlib
        tqdm
        json

2. Random Seeds
    The code uses the following seeds:
        Training seed: 42
        Eval seed: 123
        Set using: seed_all(seed)


3. Commands Used:
    1. Training :
        train(
        epochs=25,
        batch_size=128,
        lr=2e-4,
        steps=400,
        sample_every=400,
        sample_steps=50,
        center_box=12,
        dc_repeats=2,
        dc_fixed_z=True,
        pred="v",
        p2_k=1.0,
        p2_gamma=1.0,
        hole_weight=5.0,
        seed=42,
        clip_grad=1.0,
        ema_decay=0.999,
        warmup_steps=1000,
        self_cond=True,
        eta=0.0,
        coord_conv=True,
        grad_accum=1,
    )
    -> This command give 3 images as output.(i.e last.pt, panel_final.png, panel_(intermediate_names).png) 

    2. Sampling:

        sample_cmd(
            ckpt="outputs/inpaint/last.pt",
            n=16,
            steps=400,
            center_box=12,
            dc_repeats=2,
            dc_fixed_z=True,
            pred="v",
            self_cond=True,
            init_from_y=True,
            eta=0.0,
            coord_conv=True,
        )

        from IPython.display import Image, display
        display(Image(filename="outputs/inpaint/samples.png"))

        -> This commands outputs a sample.png file.

    3. Evaluation:
        eval_cmd(
        ckpt="outputs/inpaint/last.pt",
        batch_size=256,
        n_eval=500,
        steps=50,
        center_box=12,
        dc_repeats=2,
        dc_fixed_z=True,
        pred="v",
        self_cond=True,
        init_from_y=True,
        eta=0.0,
        coord_conv=True,
        seed=123,
    )

    -> This command outputs a JSON file which contains psnr_hole, and l1_hole.


 

 